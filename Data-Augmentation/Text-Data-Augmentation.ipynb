{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Data Augmentation Using NLPAUG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpaug\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from nlpaug) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from nlpaug) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from nlpaug) (2.32.2)\n",
      "Collecting gdown>=4.0.0 (from nlpaug)\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.66.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\amamo\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2024.7.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amamo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\amamo\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\amamo\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.6)\n",
      "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: gdown, nlpaug\n",
      "Successfully installed gdown-5.2.0 nlpaug-1.1.11\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpaug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.flow as naf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keyboard**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenter that applies typo error simulation to textual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "['I went ahopLihg Today, and my trolly was filled !(th BanSnXc. I Qls( had food at burrkr palQxe']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace'\n",
    "\n",
    "aug = nac.KeyboardAug(name='Keyboard_Aug', aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3, \n",
    "                      aug_word_min=1, aug_word_max=10, stopwords=None, tokenizer=None, reverse_tokenizer=None, \n",
    "                      include_special_char=True, include_numeric=True, include_upper_case=True, lang='en', verbose=0, \n",
    "                      stopwords_regex=None, model_path=None, min_char=4)\n",
    " \n",
    "test_sentence_aug = aug.augment(test_sentence)\n",
    "print(test_sentence)\n",
    "print(test_sentence_aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optical Character Recognition (OCR)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Augmenter that applies OCR error simulation to textual input.\n",
    "- For example, OCR may recognize ‘I’ as ‘1’ incorrectly, or ‘0’ as ‘o’ or ‘O’.\n",
    "- Pre-defined OCR mapping is leveraged to replace a character with a possible OCR error.\n",
    "- Solving the out of vocabulary (OOV) problem, that is Out of vocabulary words are words that are not in the training set, but appear in the test set, real data.\n",
    "- The main problem is that the model assigns a probability zero to out of vocabulary words resulting in a zero likelihood.\n",
    "    - This is a common problem, especially when you have not trained on a smaller data set.\n",
    "    - So, to overcome this we can use models like BERT and GPT (Generative Pre-trained Transformer models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "['1 went 8h0ppin9 Today, and my trolly wa8 filled with Bananas. I also had food at burgur palace']\n"
     ]
    }
   ],
   "source": [
    "aug = nac.OcrAug(name='OCR_Aug', aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3, aug_word_min=1, \n",
    "                 aug_word_max=10, stopwords=None, tokenizer=None, reverse_tokenizer=None, verbose=0, stopwords_regex=None, \n",
    "                 min_char=1)\n",
    " \n",
    "test_sentence_aug = aug.augment(test_sentence)\n",
    "print(test_sentence)\n",
    "print(test_sentence_aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenter that applies random character error to textual input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "['I went LhopPiSg Toamy, and my tr+lLy was filled wQSh BaSRpas. I alS# had food at burgur palace']\n"
     ]
    }
   ],
   "source": [
    "aug = nac.RandomCharAug(\n",
    "    action='substitute',\n",
    "    name='RandomChar_Aug',\n",
    "    aug_char_min=1,\n",
    "    aug_char_max=10,\n",
    "    aug_char_p=0.3,\n",
    "    aug_word_p=0.3,\n",
    "    aug_word_min=1,\n",
    "    aug_word_max=10,\n",
    "    include_upper_case=True,\n",
    "    include_lower_case=True,\n",
    "    include_numeric=True,\n",
    "    min_char=4,\n",
    "    swap_mode='adjacent',\n",
    "    spec_char='!@#$%^&*()_+',\n",
    "    stopwords=None,\n",
    "    tokenizer=None,\n",
    "    reverse_tokenizer=None,\n",
    "    verbose=0,\n",
    "    stopwords_regex=None,\n",
    "    candidates=None  # Corrected here\n",
    ")\n",
    "\n",
    "test_sentence_aug = aug.augment(test_sentence)\n",
    "print(test_sentence)\n",
    "print(test_sentence_aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Level Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from character enhancement, word-level is also crucial. To insert and substitute equivalent words, we use word2vec, GloVe, fast text, BERT, and wordnet. Word2vecAug, GloVeAug, and FasttextAug use word embeddings to replace the original word with the most equivalent set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "['I went Shopping Today, and my trolly was filled with Bananas. Atomic number 53 besides had food at burgur palace']\n"
     ]
    }
   ],
   "source": [
    "aug = naw.SynonymAug(aug_src='wordnet', model_path=None, name='Synonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', \n",
    "                     stopwords=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, force_reload=False, \n",
    "                     verbose=0)\n",
    " \n",
    "test_sentence_aug = aug.augment(test_sentence)\n",
    "print(test_sentence)\n",
    "print(test_sentence_aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Antonym**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenter that applies semantic meaning based on textual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very beautiful\n",
      "['very ugly']\n"
     ]
    }
   ],
   "source": [
    "aug = naw.AntonymAug(name='Antonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', stopwords=None, tokenizer=None, \n",
    "                     reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n",
    " \n",
    "test_sentence_aug = aug.augment(\"very beautiful\")\n",
    "print(\"very beautiful\")\n",
    "print(test_sentence_aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenter that applies random word operation to textual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "['I Today, and my was filled with. I had at burgur palace']\n"
     ]
    }
   ],
   "source": [
    "aug = naw.RandomWordAug(action='delete', name='RandomWord_Aug', aug_min=1, aug_max=10, aug_p=0.3, stopwords=None, \n",
    "                        target_words=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n",
    " \n",
    "test_sentence_aug = aug.augment(test_sentence)\n",
    "print(test_sentence)\n",
    "print(test_sentence_aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenter that applies spelling error simulation to textual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "[\"I'd want Sopping Today, and mmy trolly was filles with Bananas. e also had food at burgur palace\"]\n"
     ]
    }
   ],
   "source": [
    "aug = naw.SpellingAug(dict_path=None, name='Spelling_Aug', aug_min=1, aug_max=10, aug_p=0.3, stopwords=None, \n",
    "                      tokenizer=None, reverse_tokenizer=None, include_reverse=True, stopwords_regex=None, verbose=0)\n",
    " \n",
    "test_sentence_aug = aug.augment(test_sentence)\n",
    "print(test_sentence)\n",
    "print(test_sentence_aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenter that apply word splitting operation to textual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "['I went Sh opping Today, and my trolly was fi lled w ith Bananas. I al so had food at bu rgur pa lace']\n"
     ]
    }
   ],
   "source": [
    "aug = naw.SplitAug(name='Split_Aug', aug_min=1, aug_max=10, aug_p=0.3, min_char=4, stopwords=None, tokenizer=None, \n",
    "                   reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n",
    " \n",
    "test_sentence_aug = aug.augment(test_sentence)\n",
    "print(test_sentence)\n",
    "print(test_sentence_aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flow Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this type of augmentation, we can make use of multiple augmenters at once. Sequential and sometimes pipelines are used to connect augmenters in order to make use of many augmentations. A single text can be sent through multiple augmenters to yield a wide range of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amamo\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "I went Shopping Today, and my trolly was filled with Bananas. I also had food at burgur palace\n",
      "Augmented Text:\n",
      "['nowadays i went shopping twice today, and inside my trolly cupboard was filled with chopped bananas. yesterday i also had food sold at burgur palace']\n",
      "['luckily i went off shopping today, and my trolly closet was filled mostly with canned bananas. i had also had cooked food at burgur palace']\n",
      "['thankfully i went through shopping today, and my favourite trolly was filled solely with fresh bananas. i also still had food at our burgur palace']\n",
      "['i went shopping today, smiling and my trolly belly was filled with bananas. but i have also recently had fresh food at new burgur palace']\n",
      "['i went about shopping today, and my trolly was constantly filled with delicious bananas. i has also had two food shops at burgur rahman palace']\n"
     ]
    }
   ],
   "source": [
    "TOPK=20 #default=100\n",
    "ACT = 'insert' #\"substitute\"\n",
    " \n",
    "aug_bert = naw.ContextualWordEmbsAug(\n",
    "    model_path='distilbert-base-uncased', \n",
    "    #device='cuda',\n",
    "    action=ACT, top_k=TOPK)\n",
    "print(\"Original:\")\n",
    "print(test_sentence)\n",
    "print(\"Augmented Text:\")\n",
    "for ii in range(5):\n",
    "    augmented_text = aug_bert.augment(test_sentence)\n",
    "    print(augmented_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
